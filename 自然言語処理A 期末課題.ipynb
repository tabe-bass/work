{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ナイーブベイズテキスト分類"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今回使用するデータセットはscikit-learnのニュースデータセット"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipykernel in c:\\users\\tabetabe\\anaconda3\\lib\\site-packages (5.1.0)\n",
      "Requirement already satisfied: jupyter-client in c:\\users\\tabetabe\\anaconda3\\lib\\site-packages (from ipykernel) (5.2.4)\n",
      "Requirement already satisfied: traitlets>=4.1.0 in c:\\users\\tabetabe\\anaconda3\\lib\\site-packages (from ipykernel) (4.3.2)\n",
      "Requirement already satisfied: ipython>=5.0.0 in c:\\users\\tabetabe\\anaconda3\\lib\\site-packages (from ipykernel) (7.2.0)\n",
      "Requirement already satisfied: tornado>=4.2 in c:\\users\\tabetabe\\anaconda3\\lib\\site-packages (from ipykernel) (5.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\tabetabe\\anaconda3\\lib\\site-packages (from jupyter-client->ipykernel) (2.7.5)\n",
      "Requirement already satisfied: pyzmq>=13 in c:\\users\\tabetabe\\anaconda3\\lib\\site-packages (from jupyter-client->ipykernel) (17.1.2)\n",
      "Requirement already satisfied: jupyter-core in c:\\users\\tabetabe\\anaconda3\\lib\\site-packages (from jupyter-client->ipykernel) (4.4.0)\n",
      "Requirement already satisfied: ipython-genutils in c:\\users\\tabetabe\\anaconda3\\lib\\site-packages (from traitlets>=4.1.0->ipykernel) (0.2.0)\n",
      "Requirement already satisfied: six in c:\\users\\tabetabe\\anaconda3\\lib\\site-packages (from traitlets>=4.1.0->ipykernel) (1.12.0)\n",
      "Requirement already satisfied: decorator in c:\\users\\tabetabe\\anaconda3\\lib\\site-packages (from traitlets>=4.1.0->ipykernel) (4.3.0)\n",
      "Requirement already satisfied: backcall in c:\\users\\tabetabe\\anaconda3\\lib\\site-packages (from ipython>=5.0.0->ipykernel) (0.1.0)\n",
      "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in c:\\users\\tabetabe\\anaconda3\\lib\\site-packages (from ipython>=5.0.0->ipykernel) (2.0.7)\n",
      "Requirement already satisfied: jedi>=0.10 in c:\\users\\tabetabe\\anaconda3\\lib\\site-packages (from ipython>=5.0.0->ipykernel) (0.13.2)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\tabetabe\\anaconda3\\lib\\site-packages (from ipython>=5.0.0->ipykernel) (0.7.5)\n",
      "Requirement already satisfied: pygments in c:\\users\\tabetabe\\anaconda3\\lib\\site-packages (from ipython>=5.0.0->ipykernel) (2.3.1)\n",
      "Requirement already satisfied: setuptools>=18.5 in c:\\users\\tabetabe\\anaconda3\\lib\\site-packages (from ipython>=5.0.0->ipykernel) (40.6.3)\n",
      "Requirement already satisfied: colorama; sys_platform == \"win32\" in c:\\users\\tabetabe\\anaconda3\\lib\\site-packages (from ipython>=5.0.0->ipykernel) (0.4.1)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\tabetabe\\anaconda3\\lib\\site-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython>=5.0.0->ipykernel) (0.1.7)\n",
      "Requirement already satisfied: parso>=0.3.0 in c:\\users\\tabetabe\\anaconda3\\lib\\site-packages (from jedi>=0.10->ipython>=5.0.0->ipykernel) (0.3.1)\n",
      "Requirement already satisfied: mecab-python-windows in c:\\users\\tabetabe\\anaconda3\\lib\\site-packages (0.996.3)\n",
      "Requirement already satisfied: natto-py in c:\\users\\tabetabe\\anaconda3\\lib\\site-packages (0.9.0)\n",
      "Requirement already satisfied: cffi in c:\\users\\tabetabe\\anaconda3\\lib\\site-packages (from natto-py) (1.11.5)\n",
      "Requirement already satisfied: pycparser in c:\\users\\tabetabe\\anaconda3\\lib\\site-packages (from cffi->natto-py) (2.19)\n"
     ]
    }
   ],
   "source": [
    "#必要なライブラリをインポート\n",
    "!pip install ipykernel\n",
    "!pip install mecab-python-windows\n",
    "!pip install natto-py\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import  fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#データの取得\n",
    "data = fetch_20newsgroups()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "#ニュースの分類を確認\n",
    "print(data.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#学習データとテストデータを準備\n",
    "train = fetch_20newsgroups(subset='train')\n",
    "test = fetch_20newsgroups(subset='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Count = 11314\n",
      "Test Data Count = 7532\n"
     ]
    }
   ],
   "source": [
    "#データ数の確認\n",
    "print(\"Train Data Count =\", len(train.data))\n",
    "print(\"Test Data Count =\", len(test.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'str'>\n",
      "From: guykuo@carson.u.washington.edu (Guy Kuo)\n",
      "Subject: SI Clock Poll - Final Call\n",
      "Summary: Final call for SI clock reports\n",
      "Keywords: SI,acceleration,clock,upgrade\n",
      "Article-I.D.: shelley.1qvfo9INNc3s\n",
      "Organization: University of Washington\n",
      "Lines: 11\n",
      "NNTP-Posting-Host: carson.u.washington.edu\n",
      "\n",
      "A fair number of brave souls who upgraded their SI clock oscillator have\n",
      "shared their experiences for this poll. Please send a brief message detailing\n",
      "your experiences with the procedure. Top speed attained, CPU rated speed,\n",
      "add on cards and adapters, heat sinks, hour of usage per day, floppy disk\n",
      "functionality with 800 and 1.4 m floppies are especially requested.\n",
      "\n",
      "I will be summarizing in the next two days, so please add to the network\n",
      "knowledge base if you have done the clock upgrade and haven't answered this\n",
      "poll. Thanks.\n",
      "\n",
      "Guy Kuo <guykuo@u.washington.edu>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#データ内容を確認\n",
    "print (type(train.data))\n",
    "print(type(train.data[1]))\n",
    "print(train.data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comp.sys.mac.hardware\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "print(train.target_names[train.target[1]])\n",
    "print(train.target[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上の記事はcomp.sys.mac.hardwareというニュースグループの4番目の記事であることが分かる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#文書をTf-Idfベクトルに変換する\n",
    "from natto import MeCab\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.6, max_features=200000, min_df=0.01, \n",
    "                                   tokenizer=lambda x: x,\n",
    "                                   preprocessor=lambda x: x,\n",
    "                                   use_idf=True, ngram_range=(1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    tokens = []\n",
    "    with MeCab('-F%f[0],%f[6]') as nm:\n",
    "        for n in nm.parse(text, as_nodes=True):\n",
    "            # ignore any end-of-sentence nodes\n",
    "            if not n.is_eos() and n.is_nor():\n",
    "                klass, word = n.feature.split(',', 1)\n",
    "                if klass in ['名詞', '形容詞', '形容動詞', '動詞']:\n",
    "                    tokens.append(word)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#パイプラインを使用\n",
    "model = make_pipeline(TfidfVectorizer(), MultinomialNB())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('tfidfvectorizer', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth...   vocabulary=None)), ('multinomialnb', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#学習を実行\n",
    "model.fit(train.data, train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy = 0.933\n",
      " Test accuracy = 0.774\n"
     ]
    }
   ],
   "source": [
    "#作成したモデルから評価を実行\n",
    "print('Train accuracy = %.3f' % model.score(train.data, train.target))\n",
    "print(' Test accuracy = %.3f' % model.score(test.data, test.target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#任意のテキストデータを入力しグループの予測を行う\n",
    "def predicted_group(s, train=train, model=model):\n",
    "    pred = model.predict([s])\n",
    "    return train.target_names[pred[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sci.space'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_group('the test most likely had some nuclear dimension.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上の例文は核実験についてのニュースから引用したものであるので分類としては誤りであると考えられる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'soc.religion.christian'"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_group('internet connections so fast they will support an entirely new way of life.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "こちらの分類も、インターネット回線について扱っているニュースからの1文を宗教関連に分類しているため誤分類。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rec.sport.baseball'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_group('The 13 extra-base hits set a franchise record.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このニュース記事はメジャーリーグのニュース記事から引用したものであるので正しく分類されている。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "・考察\n",
    "　分類予測においては、3つ目の野球のニュースのように\"extre base hits\"などの特定しやすいワードが入っていると分類が容易であるが、1つ目、2つ目のように一般的な文章を分類する場合はまだ精度に問題があるように見られる。また、訓練データとテストデータの評価に差が出ていることや実際の分類予測の実践でも誤分類が多く見られたことから過学習が起こっている可能性も考えられる。過学習への対策としては、正則化を行うこと、訓練データを増やすことなどが挙げられる。\n",
    " また、今回はデータの前処理とモデルの生成をscikit-learnのpipelineを用いて行った。そのため、今回のデータ分類のために前処理・推論などのオブジェクトを自作しpipelineに組み込むことができればより高い精度での分類が可能になると予測する。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
